{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37bf201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\")) # append calibration folder to path for McEval import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12aa738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in\n",
    "path_mceval = \"/data/tyler/dok/viola/exchange/robin/results/Qwen/Qwen2.5-Coder-32B-Instruct/results.jsonl\"\n",
    "path_mceval_logprobs = \"/data/tyler/dok/viola/exchange/robin/results/Qwen/Qwen2.5-Coder-32B-Instruct/token_scores.pt\"\n",
    "path_mceval_evaluation = \"/data/tyler/dok/viola/exchange/robin/results/Qwen/Qwen2.5-Coder-32B-Instruct/evaluation/details.jsonl\"\n",
    "# out\n",
    "path_mceval_processed = \"output/McEval/Qwen2.5-Coder-32B-Instruct/preprocessed/mceval_results.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3430775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from McEval.eval.extract import extract\n",
    "\n",
    "def prepare_mceval_sample(sample: dict):\n",
    "    language = sample['task_id'].split('/')[0]\n",
    "    id = sample[\"task_id\"] + \"_\" + str(sample[\"generation\"])\n",
    "    try:\n",
    "        code = extract(sample[\"raw_generation\"][0], sample, language)\n",
    "    except:\n",
    "        print(f'+++++ Extract {sample[\"task_id\"]} failed')\n",
    "        code = \"\"\n",
    "    return {\n",
    "        \"id\": id,\n",
    "        \"name\": sample[\"entry_point\"],\n",
    "        \"prompt\": sample[\"instruction\"],\n",
    "        \"program\": code,\n",
    "        \"language\": language,\n",
    "        \"output_size\": len(sample[\"raw_generation\"][0]),\n",
    "        \"difficulty\": sample[\"level\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d57135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mceval_evaluation(input_path=path_mceval_evaluation, max_samples=None):\n",
    "    i = 0\n",
    "    results = {}\n",
    "    with open(input_path, \"r\") as f:   \n",
    "        for line in f:\n",
    "            if max_samples and i>=max_samples:\n",
    "                break\n",
    "             # strip leading junk before first { or [\n",
    "            if \"{\" in line or \"[\" in line:\n",
    "                i = min(\n",
    "                    [idx for idx in (line.find(\"{\"), line.find(\"[\")) if idx != -1]\n",
    "                )\n",
    "                line = line[i:]\n",
    "            chunk = json.loads(line)\n",
    "            for evaluation in chunk:\n",
    "                results[evaluation['task_id'] + \"_\" + str(evaluation['generation'])] = evaluation['pass']\n",
    "       \n",
    "            i+=1\n",
    "    return results\n",
    "\n",
    "evaluation_results = process_mceval_evaluation(max_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ca1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidates for code slicing: 73597, 13874, 19324\n",
    "\n",
    "STOP_TOKENS = ['<|endoftext|>', '<|im_end|>' ]\n",
    "\n",
    "def clean_logprob_sequence(logprobs_list: List[List]) -> np.ndarray:\n",
    "    # trim & convert to np array\n",
    "    # Trim at first stop token or first -inf prob. TODO check again what's wrong here\n",
    "    stop_candidates = ([i for i, (_, tok, _) in enumerate(logprobs_list) if tok in STOP_TOKENS] \n",
    "                       + [i for i, (_, _, prob) in enumerate(logprobs_list) if prob == float('-inf')])\n",
    "    if len(stop_candidates) > 0:\n",
    "        idx = min(stop_candidates)\n",
    "    else:\n",
    "        idx = len(logprobs_list) \n",
    "    \n",
    "    result = np.array([prob for _, _, prob in logprobs_list[:idx]])\n",
    "    # clean remaining -inf values\n",
    "    #result = np.where(np.isneginf(result), -10000, result) #check this!\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2974b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load logprobs\n",
    "logprob_data = torch.load(path_mceval_logprobs, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abaa3064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AWK': 0, 'C#': 0, 'Erlang': 0, 'F#': 0, 'Visual Basic': 0, 'Markdown': 0}\n"
     ]
    }
   ],
   "source": [
    "broken_languages = ['AWK', 'C#', 'Erlang', 'F#', 'Visual Basic', 'Markdown'] \n",
    "# AWK -> shady evaluation, \n",
    "# Markdown -> neginf probs, \n",
    "# rest: buggy evaluation\n",
    "\n",
    "def get_logprob_info_for(sample_id: str, logprob_data: dict):\n",
    "    name, generation = sample_id.rsplit(\"_\", 1)\n",
    "    generation = int(generation)\n",
    "    if name in logprob_data and generation-1 < len(logprob_data[name]):\n",
    "        logprobs = clean_logprob_sequence(logprob_data[name][generation])\n",
    "        \n",
    "        token_count = len(logprobs)\n",
    "        cumulative_logprob = sum([float(x) for x in logprobs])\n",
    "        return logprobs.tolist(), token_count, cumulative_logprob\n",
    "    else:\n",
    "        raise ValueError(f\"Logprob data not found for sample id {sample_id}\")\n",
    "\n",
    "\n",
    "def process_mceval_results(evaluation_results: dict,\n",
    "                           logprob_data: dict,\n",
    "                           input_path: str=path_mceval, \n",
    "                           output_path: str=path_mceval_processed,\n",
    "                           max_samples: int=None):\n",
    "    i = 0\n",
    "    broken = {l:0 for l in broken_languages}\n",
    "    results = []\n",
    "    with open(input_path, \"r\") as infile, open(output_path, \"w\") as outfile:   \n",
    "        for line in infile:\n",
    "            if max_samples and i>=max_samples:\n",
    "                break\n",
    "            # strip leading junk before first { or [\n",
    "            if \"{\" in line or \"[\" in line:\n",
    "                j = min(\n",
    "                    [idx for idx in (line.find(\"{\"), line.find(\"[\")) if idx != -1]\n",
    "                )\n",
    "                line = line[j:]\n",
    "            mceval_sample = json.loads(line)\n",
    "            if mceval_sample['task_id'].split('/')[0] not in broken_languages:\n",
    "                sample = prepare_mceval_sample(mceval_sample)\n",
    "                sample['is_correct'] = evaluation_results.get(sample['id'], False)\n",
    "                logprobs, token_count, cumulative_logprob = get_logprob_info_for(sample['id'], logprob_data)\n",
    "                sample[\"token_count\"] = token_count\n",
    "                sample[\"token_logprobs\"] = logprobs\n",
    "                sample['cumulative_logprob'] = cumulative_logprob\n",
    "                \n",
    "                # DEBUG\n",
    "                if (token_count == 0\n",
    "                    or np.exp(cumulative_logprob / token_count) == 0.0):\n",
    "                    print('check me')\n",
    "                \n",
    "                \n",
    "                \n",
    "                results.append(sample)\n",
    "                outfile.write(json.dumps(sample) + \"\\n\")\n",
    "                i+=1\n",
    "            else: broken[mceval_sample['task_id'].split('/')[0]] +=1\n",
    "    print(broken)\n",
    "    return results\n",
    "\n",
    "results = process_mceval_results(evaluation_results, logprob_data, output_path=\"/dev/null\", max_samples=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LiveCodeBench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
